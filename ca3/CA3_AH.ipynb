{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepocessing data\n",
    "# Source lecture DAT200. File: rawDataInspection_01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CA3-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values!\n"
     ]
    }
   ],
   "source": [
    "# Search for missing values\n",
    "missing = np.asarray(df.isnull().sum())\n",
    "if missing.any():\n",
    "    print(\"Dataset has missing values\")\n",
    "else:\n",
    "    print('No missing values!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: Index(['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11',\n",
      "       'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21',\n",
      "       'f22', 'f23', 'f24'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# features index\n",
    "c_first = 1\n",
    "c_last = 25    # not included\n",
    "# Assign features to X matrix and corresponding labels to vector y\n",
    "X, y = df.iloc[:, c_first:c_last].values, df.iloc[:, 25]\n",
    "print(f\"Selected features:\", df.iloc[:, c_first:c_last].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the unique class labels\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters\n",
    "seed = 1\n",
    "test_size = 0.3\n",
    "\n",
    "# Splitting data with default parameters\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "# Standardizing our data to make algorithms behave better\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot decision regions. Works only when two features are selected\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # Source Python for Machine Learning ch05\n",
    "    # setup marker generator and colormap\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx2.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot examples by class\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=colors[idx],\n",
    "                    marker=markers[idx], label=cl,\n",
    "                    edgecolor='black')\n",
    "    # highlight test examples\n",
    "    if test_idx:\n",
    "        # plot all examples\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1],\n",
    "                    c='', edgecolor='black', alpha=1.0,\n",
    "                    linewidth=1, marker='o',\n",
    "                    s=100, label='test set')\n",
    "    plt.xlabel('First feature [standardized]')\n",
    "    plt.ylabel('Second feature [standardized]')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined(X_train, X_test, y_train, y_test):\n",
    "    # Stacking the data before plotting\n",
    "    X_combined = X_train + X_test\n",
    "    y_combined = y_train + y_test\n",
    "    return X_combined, y_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_test_size(classifier, X, y, test_size_list, seed, feature_extraction=False, n_components=None):\n",
    "    # Accuracy for different test_train_splits\n",
    "    for size in test_size_list:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=size, stratify=y,\n",
    "                                                            random_state=seed)\n",
    "        X_train_std = sc.fit_transform(X_train)\n",
    "        X_test_std = sc.fit_transform(X_test)\n",
    "        if feature_extraction:\n",
    "            # Dimensionality reduction through PCA\n",
    "            pca = PCA(n_components=n_components, random_state=1)\n",
    "            X_train_pca = pca.fit_transform(X_train_std)\n",
    "            X_test_pca = pca.fit_transform(X_test_std)\n",
    "            classifier.fit(X_train_pca, y_train)\n",
    "            y_pred = classifier.predict(X_test_pca)\n",
    "            print(f'Misclassified examples PCA: {(y_test != y_pred).sum()}')\n",
    "            print('Accuracy PCA: {:.3}'.format(classifier.score(X_test_pca, y_test)))\n",
    "            print(f'Test size: {size}')\n",
    "        else:\n",
    "            classifier.fit(X_train_std, y_train)\n",
    "            y_pred = classifier.predict(X_test_std)\n",
    "            print(f'Misclassified examples: {(y_test != y_pred).sum()}')\n",
    "            print('Training Accuracy: {:.3}'.format(classifier.score(X_train_std, y_train)))\n",
    "            print('Accuracy: {:.3}'.format(classifier.score(X_test_std, y_test)))\n",
    "            print(f'Test size: {size}')\n",
    "    # Note: After function call test size is the last index of test_size_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative sum of explained variances\n",
    "def plot_var_exp(n_components):\n",
    "    pca = PCA(n_components=n_components, random_state=1)\n",
    "    pca.fit_transform(X_train_std)\n",
    "    pca.fit_transform(X_test_std)\n",
    "    var_exp = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(var_exp)\n",
    "    plt.bar(range(1, n_components + 1), var_exp, alpha=0.5, align='center',\n",
    "            label='Individual explained variances')\n",
    "    plt.step(range(1, n_components + 1), cum_var_exp, where='mid',\n",
    "             label='Cumulative explained variances')\n",
    "    plt.xlabel('Explained variance ratio')\n",
    "    plt.ylabel('Principal component index')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C:  1\n",
      "Max acurracy score: 0.492\n",
      "Test size:  0.3\n"
     ]
    }
   ],
   "source": [
    "# Fit Logistic regression and calculate accuracy for different train_test_splits\n",
    "C_list = [100, 10, 1, 0.1, 0.01]\n",
    "max_score = 0\n",
    "for C in C_list:\n",
    "    lr_temp = LogisticRegression(penalty='l1', C=C, solver='liblinear', random_state=seed)\n",
    "    lr_temp.fit(X_train_std, y_train)\n",
    "    score = lr_temp.score(X_test_std, y_test)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        C_best = C\n",
    "\n",
    "print(\"Best C: \", C_best)\n",
    "print(\"Max acurracy score: {:.3}\".format(max_score))\n",
    "print(\"Test size: \", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 2142\n",
      "Training Accuracy: 0.519\n",
      "Accuracy: 0.498\n",
      "Test size: 0.6\n",
      "Misclassified examples: 1084\n",
      "Training Accuracy: 0.526\n",
      "Accuracy: 0.492\n",
      "Test size: 0.3\n",
      "Misclassified examples: 350\n",
      "Training Accuracy: 0.514\n",
      "Accuracy: 0.508\n",
      "Test size: 0.1\n",
      "Misclassified examples: 170\n",
      "Training Accuracy: 0.513\n",
      "Accuracy: 0.522\n",
      "Test size: 0.05\n",
      "Misclassified examples: 31\n",
      "Training Accuracy: 0.513\n",
      "Accuracy: 0.569\n",
      "Test size: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Fit Logistic regression and calculate accuracy for different train_test_splits\n",
    "test_size_list = [0.6, 0.3, 0.1, 0.05, 0.01]\n",
    "lr = LogisticRegression(penalty='l1', C=0.1, solver='liblinear', random_state=seed)\n",
    "fit_test_size(lr, X, y, test_size_list, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy with feature extraction\n",
    "fit_test_size(lr, X, y, test_size_list, seed, feature_extraction=True, n_components=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into the data\n",
    "\n",
    "comment: Might skip this in final version\n",
    "\n",
    "* Search for correlations\n",
    "* Look for outliers\n",
    "* Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.iloc[:, 1:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Descriptive statistics\n",
    "# =============================================================================\n",
    "df_X.describe()\n",
    "#df.iloc[:, 16].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for positive values f16\n",
    "\n",
    "positive_f16 = np.where(df['f16']>0, True, False)\n",
    "# count True\n",
    "positive_f16.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with zero values\n",
    "\n",
    "extract_positive_f16 = df[df['f16']>0]['f16']\n",
    "extract_positive_f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram positive f16\n",
    "extract_positive_f16.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for posive f16\n",
    "\n",
    "extract_positive_f16.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Histograms\n",
    "# =============================================================================\n",
    "\n",
    "df.iloc[:, 16].hist()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes AH: Possible outliers f16, very high max relative to the rest.\n",
    "Also a lot of zero values more than two-thirds. Consider dropping the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Density plots\n",
    "# =============================================================================\n",
    "\n",
    "df['f16'].plot(kind='density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values for f16 in descending order\n",
    "\n",
    "df['f16'].sort_values(ascending=False)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['f16'], ascending=False)[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Skip the six first datapoints from the sorting above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Plot correlation matrix\n",
    "# =============================================================================\n",
    "\n",
    "# plot correlation matrix for the first four features\n",
    "df_sub = df.iloc[:, 21:25]\n",
    "correlations = df_sub.corr()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0, 5, 1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(list(df_sub.columns))\n",
    "ax.set_yticklabels(list(df_sub.columns))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_f15f16 = df.iloc[:, [15, 16]].corr()\n",
    "corr_f15f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations all f16\n",
    "corr_all = df_X.corr()\n",
    "corr_all.iloc[:, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing\n",
    "\n",
    "sc = StandardScaler()\n",
    "df_X_std = sc.fit_transform(df_X)\n",
    "df_X_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_X_std[14], df_X_std[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
